{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Automated Machine Learning\n",
    "This is the code for the paper entitled \"**[IoT Data Analytics in Dynamic Environments: From An Automated Machine Learning Perspective](https://arxiv.org/abs/2209.08018)**\" published in *Engineering Applications of Artificial Intelligence* (Elsevier's Journal, IF:7.8).<br>\n",
    "Authors: Li Yang (lyang339@uwo.ca) and Abdallah Shami (Abdallah.Shami@uwo.ca)<br>\n",
    "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University\n",
    "\n",
    "L. Yang and A. Shami, \"IoT Data Analytics in Dynamic Environments: From An Automated Machine Learning Perspective\", *Engineering Applications of Artificial Intelligence*, vol. 116, pp. 1-33, 2022, doi: https://doi.org/10.1016/j.engappai.2022.105366."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Part 1: Automated Offline/Static/Batch Learning\n",
    "Batch learning: Batch learning methods analyze static data in batches and often need access to the entire dataset prior to model training. Traditional ML algorithms can effectively solve batch learning tasks. Although batch learning models often achieve high performance due to their ability to learn diverse data patterns, it is often difficult to update these models once created. Therefore, batch learning faces two significant challenges: model degradation and data unavailability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: CICIDS2017\n",
    "A subset of the network traffic data randomly sampled from the [CICIDS2017 dataset](https://www.unb.ca/cic/datasets/ids-2017.html).  \n",
    "\n",
    "The Canadian Institute for Cybersecurity Intrusion Detection System 2017 (CICIDS2017) dataset has the most updated network threats. The CICIDS2017 dataset is close to real-world network data since it has a large amount of network traffic data, a variety of network features, various types of attacks, and highly imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import shapiro\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sampled CICIDS2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = r'utils\\dataset_files'\n",
    "df = pd.read_csv(os.path.join(dataset_directory,\"Test_DS.csv\"))\n",
    "df.columns= ['Timestamp', 'CAN_ID', 'RTR', 'DLC', 'Data0', 'Data1', 'Data2', 'Data3', 'Data4', 'Data5', 'Data6', 'Data7', 'Label','Anomaly_Label',\\\n",
    "        'Mean', 'Median','Skew', 'Kurtosis', 'Variance', 'Standard_deviation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>CAN_ID</th>\n",
       "      <th>RTR</th>\n",
       "      <th>DLC</th>\n",
       "      <th>Data0</th>\n",
       "      <th>Data1</th>\n",
       "      <th>Data2</th>\n",
       "      <th>Data3</th>\n",
       "      <th>Data4</th>\n",
       "      <th>Data5</th>\n",
       "      <th>Data6</th>\n",
       "      <th>Data7</th>\n",
       "      <th>Label</th>\n",
       "      <th>Anomaly_Label</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Standard_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008960</td>\n",
       "      <td>870.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.750</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.591948</td>\n",
       "      <td>-1.328752</td>\n",
       "      <td>6842.214286</td>\n",
       "      <td>82.717678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011995</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.023583</td>\n",
       "      <td>4.221153</td>\n",
       "      <td>6230.214286</td>\n",
       "      <td>78.931706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011997</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.875</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.256800</td>\n",
       "      <td>1.354502</td>\n",
       "      <td>7266.125000</td>\n",
       "      <td>85.241568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014066</td>\n",
       "      <td>1694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.375</td>\n",
       "      <td>47.5</td>\n",
       "      <td>0.637440</td>\n",
       "      <td>-1.421334</td>\n",
       "      <td>5544.553571</td>\n",
       "      <td>74.461759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018414</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.500</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.995326</td>\n",
       "      <td>-0.749078</td>\n",
       "      <td>6530.857143</td>\n",
       "      <td>80.813719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923723</th>\n",
       "      <td>347.270475</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>1.388730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923724</th>\n",
       "      <td>347.276351</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153.000</td>\n",
       "      <td>185.5</td>\n",
       "      <td>-0.603115</td>\n",
       "      <td>-1.337754</td>\n",
       "      <td>7804.285714</td>\n",
       "      <td>88.341868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923725</th>\n",
       "      <td>347.281315</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923726</th>\n",
       "      <td>347.297869</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.500</td>\n",
       "      <td>132.5</td>\n",
       "      <td>-0.036447</td>\n",
       "      <td>-1.757365</td>\n",
       "      <td>8990.571429</td>\n",
       "      <td>94.818624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923727</th>\n",
       "      <td>347.307637</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97.750</td>\n",
       "      <td>106.5</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>-1.860437</td>\n",
       "      <td>5821.071429</td>\n",
       "      <td>76.295946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923728 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Timestamp  CAN_ID  RTR  DLC  Data0  Data1  Data2  Data3  Data4  \\\n",
       "0          0.008960   870.0  0.0  8.0  129.0   29.0  200.0    2.0    1.0   \n",
       "1          0.011995  1634.0  0.0  8.0   78.0  224.0    0.0    0.0   64.0   \n",
       "2          0.011997   208.0  0.0  8.0   82.0  119.0    4.0   96.0    1.0   \n",
       "3          0.014066  1694.0  0.0  8.0    4.0   64.0    4.0  125.0   31.0   \n",
       "4          0.018414   186.0  0.0  8.0    6.0  184.0   83.0  196.0   16.0   \n",
       "...             ...     ...  ...  ...    ...    ...    ...    ...    ...   \n",
       "1923723  347.270475  1201.0  0.0  8.0    0.0    0.0    3.0    0.0    0.0   \n",
       "1923724  347.276351   339.0  0.0  8.0   44.0  179.0   98.0   18.0  251.0   \n",
       "1923725  347.281315  1440.0  0.0  8.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1923726  347.297869   356.0  0.0  8.0  196.0  245.0   47.0  130.0  238.0   \n",
       "1923727  347.307637  1201.0  0.0  8.0   78.0  186.0  135.0  195.0  138.0   \n",
       "\n",
       "         Data5  Data6  Data7  Label  Anomaly_Label     Mean  Median      Skew  \\\n",
       "0         71.0  206.0   56.0    2.0            0.0   86.750    63.5  0.591948   \n",
       "1          0.0    0.0    0.0    2.0            0.0   45.750     0.0  2.023583   \n",
       "2          1.0  240.0    0.0    2.0            0.0   67.875    43.0  1.256800   \n",
       "3        192.0   21.0  162.0    2.0            0.0   75.375    47.5  0.637440   \n",
       "4          0.0    3.0   52.0    2.0            0.0   67.500    34.0  0.995326   \n",
       "...        ...    ...    ...    ...            ...      ...     ...       ...   \n",
       "1923723    0.0    0.0    3.0    2.0            0.0    0.750     0.0  1.440165   \n",
       "1923724  236.0  192.0  206.0    0.0            1.0  153.000   185.5 -0.603115   \n",
       "1923725    0.0    0.0    0.0    2.0            0.0    0.000     0.0  0.000000   \n",
       "1923726    6.0  135.0   23.0    0.0            1.0  127.500   132.5 -0.036447   \n",
       "1923727   22.0   27.0    1.0    0.0            1.0   97.750   106.5  0.009349   \n",
       "\n",
       "         Kurtosis     Variance  Standard_deviation  \n",
       "0       -1.328752  6842.214286           82.717678  \n",
       "1        4.221153  6230.214286           78.931706  \n",
       "2        1.354502  7266.125000           85.241568  \n",
       "3       -1.421334  5544.553571           74.461759  \n",
       "4       -0.749078  6530.857143           80.813719  \n",
       "...           ...          ...                 ...  \n",
       "1923723  0.000000     1.928571            1.388730  \n",
       "1923724 -1.337754  7804.285714           88.341868  \n",
       "1923725  0.000000     0.000000            0.000000  \n",
       "1923726 -1.757365  8990.571429           94.818624  \n",
       "1923727 -1.860437  5821.071429           76.295946  \n",
       "\n",
       "[1923728 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Automated Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Transformation/Encoding\n",
    "Automatically identify and transform string/text features into numerical features to make the data more readable by ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the automated data encoding function\n",
    "def Auto_Encoding(df):\n",
    "    cat_features=[x for x in df.columns if df[x].dtype==\"object\"] ## Find string/text features\n",
    "    le=LabelEncoder()\n",
    "    for col in cat_features:\n",
    "        if col in df.columns:\n",
    "            i = df.columns.get_loc(col)\n",
    "            # Transform to numerical features\n",
    "            df.iloc[:,i] = df.apply(lambda i:le.fit_transform(i.astype(str)), axis=0, result_type='expand')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Auto_Encoding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Imputation\n",
    "Detect and impute missing values to improve data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the automated data imputation function\n",
    "def Auto_Imputation(df):\n",
    "    if df.isnull().values.any() or np.isinf(df).values.any(): # if there is any empty or infinite values\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(0, inplace = True)  # Replace empty values with zeros; there are other imputation methods discussed in the paper\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Auto_Imputation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated normalization\n",
    "Normalize the range of features to a similar scale to improve data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Auto_Normalization(df):\n",
    "    stat, p = shapiro(df)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    numeric_features = df.drop(['Label'],axis = 1).dtypes[df.dtypes != 'object'].index\n",
    "    \n",
    "    # The selection strategy is based on the following article: \n",
    "    # https://medium.com/@kumarvaishnav17/standardization-vs-normalization-in-machine-learning-3e132a19c8bf\n",
    "    # Check if the data distribution follows a Gaussian/normal distribution\n",
    "    # If so, select the Z-score normalization method; otherwise, select the min-max normalization\n",
    "    # Details are in the paper\n",
    "    if p > alpha:\n",
    "        print('Sample looks Gaussian (fail to reject H0)')\n",
    "        df[numeric_features] = df[numeric_features].apply(\n",
    "            lambda x: (x - x.mean()) / (x.std()))\n",
    "        print('Z-score normalization is automatically chosen and used')\n",
    "    else:\n",
    "        print('Sample does not look Gaussian (reject H0)')\n",
    "        df[numeric_features] = df[numeric_features].apply(\n",
    "            lambda x: (x - x.min()) / (x.max()-x.min()))\n",
    "        print('Min-max normalization is automatically chosen and used')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics=0.225, p=0.000\n",
      "Sample does not look Gaussian (reject H0)\n",
      "Min-max normalization is automatically chosen and used\n"
     ]
    }
   ],
   "source": [
    "df=Auto_Normalization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train-test split\n",
    "Split the dataset into the training and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label','Anomaly_Label'],axis=1)\n",
    "y = df['Anomaly_Label']\n",
    "\n",
    "# Here we used the 80%/20% split, it can be changed based on specific tasks\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=False,random_state = 0)\n",
    "X_new, X_subset, y_new, y_subset = train_test_split(X, y, train_size=500000, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new,y_new, train_size = 0.1, test_size = 0.9, shuffle=False,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated data balancing\n",
    "Generate minority class samples to solve class-imbalance and improve data quality.  \n",
    "Synthetic Minority Over-sampling Technique (SMOTE) method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    40707\n",
       "1.0     9293\n",
       "Name: Anomaly_Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary data (can be modified for multi-class data with the same logic)\n",
    "def Auto_Balancing(X_train, y_train):\n",
    "    number0 = pd.Series(y_train).value_counts().iloc[0]\n",
    "    number1 = pd.Series(y_train).value_counts().iloc[1]\n",
    "    \n",
    "    if number0 > number1:\n",
    "        nlarge = number0\n",
    "    else:\n",
    "        nlarge = number1\n",
    "    \n",
    "    # evaluate whether the incoming dataset is imbalanced (the abnormal/normal ratio is smaller than a threshold (e.g., 50%)) \n",
    "    if (number1/number0 > 1.5) or (number0/number1 > 1.5):\n",
    "        smote=SMOTE(n_jobs=-1,sampling_strategy={0:nlarge, 1:nlarge})\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = Auto_Balancing(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    40707\n",
       "1.0    40707\n",
       "Name: Anomaly_Label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model learning (for Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.583%\n",
      "Precision: 73.236%\n",
      "Recall: 79.235%\n",
      "F1-score: 76.118%\n",
      "Time: 0.6835\n",
      "CPU times: total: 9.27 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg = lgb.LGBMClassifier(verbose = -1)\n",
    "lg.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = lg.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.431%\n",
      "Precision: 67.354%\n",
      "Recall: 75.519%\n",
      "F1-score: 71.20400000000001%\n",
      "Time: 13.98276\n",
      "CPU times: total: 23.3 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = rf.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.414%\n",
      "Precision: 24.557000000000002%\n",
      "Recall: 88.28399999999999%\n",
      "F1-score: 38.426%\n",
      "Time: 0.43122\n",
      "CPU times: total: 719 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = nb.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.045%\n",
      "Precision: 57.656%\n",
      "Recall: 79.211%\n",
      "F1-score: 66.73599999999999%\n",
      "Time: 62.55657\n",
      "CPU times: total: 4min 55s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "t1=time.time()\n",
    "predictions = knn.predict(X_test)\n",
    "t2=time.time()\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input,Dense,Dropout,BatchNormalization,Activation\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import keras.callbacks as kcallbacks\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "def ANN(optimizer = 'sgd',neurons=16,batch_size=1024,epochs=80,activation='relu',patience=8,loss='binary_crossentropy'):\n",
    "    K.clear_session()\n",
    "    inputs=Input(shape=(X.shape[1],))\n",
    "    x=Dense(1000)(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x=Dense(256)(inputs)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "    x=Dense(2,activation='softmax')(x)\n",
    "    model=Model(inputs=inputs,outputs=x,name='base_nlp')\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    early_stopping = EarlyStopping(monitor=\"loss\", patience = patience)# early stop patience\n",
    "    history = model.fit(X, pd.get_dummies(y).values,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks = [early_stopping],\n",
    "              verbose=0) #verbose set to 1 will show the training process\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14063/14063 [==============================] - 8s 565us/step\n",
      "Accuracy: 89.912%\n",
      "Precision: 71.04%\n",
      "Recall: 78.89399999999999%\n",
      "F1-score: 74.762%\n",
      "Time: 62.55657\n",
      "CPU times: total: 1h 48min 43s\n",
      "Wall time: 14min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ann = KerasClassifier(build_fn=ANN, verbose=0)\n",
    "ann.fit(X_train,y_train)\n",
    "predictions = ann.predict(X_test)\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Time: \"+str(round((t2-t1)/len(y_test)*1000000,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Automated Feature Engineering\n",
    "Feature selection method 1: **Information Gain (IG)**, used to remove irrelevant features to improve model efficiency  \n",
    "Feature selection method 2: **Pearson Correlation**, used to remove redundant features to improve model efficiency and accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features and select important features\n",
    "def Feature_Importance_IG(data):\n",
    "    features = data.drop(['Label'],axis=1).values  # \"Label\" should be changed to the target class variable name if different\n",
    "    labels = data['Label'].values\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(data.drop(['Label'],axis=1).columns)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    model = lgb.LGBMRegressor(verbose = -1)\n",
    "    model.fit(features, labels)\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "\n",
    "    # Sort features according to importance\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "    feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "    \n",
    "    cumulative_importance=0.90 # Only keep the important features with cumulative importance scores>=90%. It can be changed.\n",
    "\n",
    "    # Make sure most important features are on top\n",
    "    feature_importances = feature_importances.sort_values('cumulative_importance')\n",
    "\n",
    "    # Identify the features not needed to reach the cumulative_importance\n",
    "    record_low_importance = feature_importances[feature_importances['cumulative_importance'] > cumulative_importance]\n",
    "\n",
    "    to_drop = list(record_low_importance['feature'])\n",
    "#     print(feature_importances.drop(['importance'],axis=1))\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant features\n",
    "def Feature_Redundancy_Pearson(data):\n",
    "    correlation_threshold=0.90 # Only remove features with the redundancy>90%. It can be changed\n",
    "    features = data.drop(['Label'],axis=1)\n",
    "    corr_matrix = features.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    # Dataframe to hold correlated pairs\n",
    "    record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "    # Iterate through the columns to drop\n",
    "    for column in to_drop:\n",
    "\n",
    "        # Find the correlated features\n",
    "        corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "        # Find the correlated values\n",
    "        corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "        drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "        # Record the information (need a temp df for now)\n",
    "        temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                         'corr_feature': corr_features,\n",
    "                                         'corr_value': corr_values})\n",
    "        record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "#     print(record_collinear)\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_Feature_Engineering(df):\n",
    "    drop1 = Feature_Importance_IG(df)\n",
    "    dfh1 = df.drop(columns = drop1)\n",
    "    \n",
    "    drop2 = Feature_Redundancy_Pearson(dfh1)\n",
    "    dfh2 = dfh1.drop(columns = drop2)\n",
    "    \n",
    "    return dfh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>CAN_ID</th>\n",
       "      <th>Data0</th>\n",
       "      <th>Data1</th>\n",
       "      <th>Data3</th>\n",
       "      <th>Data4</th>\n",
       "      <th>Data6</th>\n",
       "      <th>Label</th>\n",
       "      <th>Anomaly_Label</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.801449e-07</td>\n",
       "      <td>0.451245</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>0.602383</td>\n",
       "      <td>0.136227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.178308e-06</td>\n",
       "      <td>0.847510</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>0.856909</td>\n",
       "      <td>0.650107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.178504e-06</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>0.160156</td>\n",
       "      <td>0.232422</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132568</td>\n",
       "      <td>0.720585</td>\n",
       "      <td>0.384676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.381766e-06</td>\n",
       "      <td>0.878631</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>0.610471</td>\n",
       "      <td>0.127654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.808919e-06</td>\n",
       "      <td>0.096473</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.674098</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923723</th>\n",
       "      <td>3.411635e-02</td>\n",
       "      <td>0.622925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923724</th>\n",
       "      <td>3.411692e-02</td>\n",
       "      <td>0.175830</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.349609</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.389916</td>\n",
       "      <td>0.135393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923725</th>\n",
       "      <td>3.411741e-02</td>\n",
       "      <td>0.746888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923726</th>\n",
       "      <td>3.411904e-02</td>\n",
       "      <td>0.184647</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.490663</td>\n",
       "      <td>0.096540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923727</th>\n",
       "      <td>3.412000e-02</td>\n",
       "      <td>0.622925</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>0.498805</td>\n",
       "      <td>0.086997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923728 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp    CAN_ID     Data0     Data1     Data3     Data4  \\\n",
       "0        8.801449e-07  0.451245  0.251953  0.056641  0.003906  0.001953   \n",
       "1        1.178308e-06  0.847510  0.152344  0.437500  0.000000  0.125000   \n",
       "2        1.178504e-06  0.107884  0.160156  0.232422  0.187500  0.001953   \n",
       "3        1.381766e-06  0.878631  0.007812  0.125000  0.244141  0.060547   \n",
       "4        1.808919e-06  0.096473  0.011719  0.359375  0.382812  0.031250   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "1923723  3.411635e-02  0.622925  0.000000  0.000000  0.000000  0.000000   \n",
       "1923724  3.411692e-02  0.175830  0.085938  0.349609  0.035156  0.490234   \n",
       "1923725  3.411741e-02  0.746888  0.000000  0.000000  0.000000  0.000000   \n",
       "1923726  3.411904e-02  0.184647  0.382812  0.478516  0.253906  0.464844   \n",
       "1923727  3.412000e-02  0.622925  0.152344  0.363281  0.380859  0.269531   \n",
       "\n",
       "            Data6  Label  Anomaly_Label      Mean      Skew  Kurtosis  \n",
       "0        0.402344    2.0            0.0  0.169434  0.602383  0.136227  \n",
       "1        0.000000    2.0            0.0  0.089355  0.856909  0.650107  \n",
       "2        0.468750    2.0            0.0  0.132568  0.720585  0.384676  \n",
       "3        0.041016    2.0            0.0  0.147217  0.610471  0.127654  \n",
       "4        0.005859    2.0            0.0  0.131836  0.674098  0.189900  \n",
       "...           ...    ...            ...       ...       ...       ...  \n",
       "1923723  0.000000    2.0            0.0  0.001465  0.753185  0.259259  \n",
       "1923724  0.375000    0.0            1.0  0.298828  0.389916  0.135393  \n",
       "1923725  0.000000    2.0            0.0  0.000000  0.497142  0.259259  \n",
       "1923726  0.263672    0.0            1.0  0.249023  0.490663  0.096540  \n",
       "1923727  0.052734    0.0            1.0  0.190918  0.498805  0.086997  \n",
       "\n",
       "[1923728 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh2 = Auto_Feature_Engineering(df)\n",
    "dfh2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split & Balancing (After Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfh2.drop(['Label','Anomaly_Label'],axis=1)\n",
    "y = dfh2['Anomaly_Label']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, shuffle=False,random_state = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = Auto_Balancing(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Automated Model Selection\n",
    "Select the best-performing model among five common machine learning models (Naive Bayes, KNN, random forest, LightGBM, and ANN/MLP) by evaluating their learning performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([('classifier', GaussianNB())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'classifier': [GaussianNB()]},\n",
    "                {'classifier': [KNeighborsClassifier()]},\n",
    "                {'classifier': [RandomForestClassifier()]},\n",
    "                {'classifier': [lgb.LGBMClassifier(verbose = -1)]},\n",
    "                {'classifier': [KerasClassifier(build_fn=ANN, verbose=0)]},\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;classifier&#x27;, GaussianNB())]),\n",
       "             param_grid=[{&#x27;classifier&#x27;: [GaussianNB()]},\n",
       "                         {&#x27;classifier&#x27;: [KNeighborsClassifier()]},\n",
       "                         {&#x27;classifier&#x27;: [RandomForestClassifier()]},\n",
       "                         {&#x27;classifier&#x27;: [LGBMClassifier(verbose=-1)]},\n",
       "                         {&#x27;classifier&#x27;: [&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x000001A6BB571700&gt;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;classifier&#x27;, GaussianNB())]),\n",
       "             param_grid=[{&#x27;classifier&#x27;: [GaussianNB()]},\n",
       "                         {&#x27;classifier&#x27;: [KNeighborsClassifier()]},\n",
       "                         {&#x27;classifier&#x27;: [RandomForestClassifier()]},\n",
       "                         {&#x27;classifier&#x27;: [LGBMClassifier(verbose=-1)]},\n",
       "                         {&#x27;classifier&#x27;: [&lt;keras.wrappers.scikit_learn.KerasClassifier object at 0x000001A6BB571700&gt;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;classifier&#x27;, GaussianNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('classifier', GaussianNB())]),\n",
       "             param_grid=[{'classifier': [GaussianNB()]},\n",
       "                         {'classifier': [KNeighborsClassifier()]},\n",
       "                         {'classifier': [RandomForestClassifier()]},\n",
       "                         {'classifier': [LGBMClassifier(verbose=-1)]},\n",
       "                         {'classifier': [<keras.wrappers.scikit_learn.KerasClassifier object at 0x000001A6BB571700>]}])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:{'classifier': RandomForestClassifier()}\n",
      "Accuracy:0.8349538145772648\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Model:\"+ str(clf.best_params_))\n",
    "print(\"Accuracy:\"+ str(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.49287271e-01, 9.23181119e+00, 3.44438641e+02, 2.47019458e+00,\n",
       "        8.56434151e+02]),\n",
       " 'std_fit_time': array([4.97836289e-03, 4.97621235e-01, 3.79760645e+01, 1.63161131e-02,\n",
       "        6.72389365e+01]),\n",
       " 'mean_score_time': array([ 0.11600556, 27.21723347,  4.55360003,  0.23275356,  5.67175727]),\n",
       " 'std_score_time': array([3.25754269e-03, 1.12521099e+01, 7.18234999e-01, 7.96020089e-03,\n",
       "        1.20933518e-01]),\n",
       " 'param_classifier': masked_array(data=[GaussianNB(), KNeighborsClassifier(),\n",
       "                    RandomForestClassifier(), LGBMClassifier(verbose=-1),\n",
       "                    <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001A6BB571700>],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': GaussianNB()},\n",
       "  {'classifier': KNeighborsClassifier()},\n",
       "  {'classifier': RandomForestClassifier()},\n",
       "  {'classifier': LGBMClassifier(verbose=-1)},\n",
       "  {'classifier': <keras.wrappers.scikit_learn.KerasClassifier at 0x1a6bb571700>}],\n",
       " 'split0_test_score': array([0.65941686, 0.61738654, 0.72372682, 0.793287  ,        nan]),\n",
       " 'split1_test_score': array([0.83115354, 0.90709455, 0.90275662, 0.90188852,        nan]),\n",
       " 'split2_test_score': array([0.76963243, 0.82205143, 0.84656891, 0.8109584 ,        nan]),\n",
       " 'split3_test_score': array([0.59612471, 0.78044679, 0.85129111, 0.72288139,        nan]),\n",
       " 'split4_test_score': array([0.55603842, 0.81584166, 0.85042561, 0.78750861,        nan]),\n",
       " 'mean_test_score': array([0.68247319, 0.78856419, 0.83495381, 0.80330478,        nan]),\n",
       " 'std_test_score': array([0.1036108 , 0.09516573, 0.0593462 , 0.05761099,        nan]),\n",
       " 'rank_test_score': array([4, 3, 1, 2, 5])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM model is the best performing machine learning model, and the best cross-validation accuracy is 98.438%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Bayesian Optimization with Tree Parzen Estimator (BO-TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 0.4/1.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.6 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 14.3 MB/s eta 0:00:00\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -------------------- ------------------- 1.0/2.0 MB 22.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.0 MB 25.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 21.6 MB/s eta 0:00:00\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 200.5/200.5 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: future in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (0.18.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (1.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (2.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from hyperopt) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\karth\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Installing collected packages: py4j, networkx, hyperopt\n",
      "Successfully installed hyperopt-0.2.7 networkx-3.0 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "! pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [57:05<00:00, 342.56s/trial, best loss: -0.9314846678068128] \n",
      "Hyperopt estimated optimum {'classifier_type': 2}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    \n",
    "    classifier_type = params['type']\n",
    "    del params['type']\n",
    "    if classifier_type == 'nb':\n",
    "        clf = GaussianNB()\n",
    "    elif classifier_type == 'knn':\n",
    "        clf = KNeighborsClassifier()\n",
    "    elif classifier_type == 'rf':\n",
    "        clf = RandomForestClassifier()\n",
    "    elif classifier_type == 'lgb':\n",
    "        clf = lgb.LGBMClassifier(verbose = -1)\n",
    "    elif classifier_type == 'ann':\n",
    "        clf = KerasClassifier(build_fn=ANN, verbose=0)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test,predictions)\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = hp.choice('classifier_type', [{'type': 'nb'},{'type': 'knn'},{'type': 'rf'},{'type': 'lgb'},{'type': 'ann'},])\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier type 3 is the LightGBM model, and the best hold-out accuracy is 99.806%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Hyperparameter Optimization\n",
    "Optimize the best performing machine learning model (lightGBM) by tuning its hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [30:04<00:00, 90.22s/trial, best loss: -0.8268782903387375] \n",
      "LightGBM: Hyperopt estimated optimum {'learning_rate': 0.0031594313778544603, 'max_depth': 47.0, 'min_child_samples': 45.0, 'n_estimators': 460.0, 'num_leaves': 1300.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': abs(float(params['learning_rate'])),\n",
    "        \"num_leaves\": int(params['num_leaves']),\n",
    "        \"min_child_samples\": int(params['min_child_samples']),\n",
    "    }\n",
    "    clf = lgb.LGBMClassifier( **params)\n",
    "    score = cross_val_score(clf, X, y, scoring='accuracy', cv=StratifiedKFold(n_splits=5)).mean()\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 500, 20),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"learning_rate\":hp.uniform('learning_rate', 0, 1),\n",
    "    \"num_leaves\":hp.quniform('num_leaves',100,2000,100),\n",
    "    \"min_child_samples\":hp.quniform('min_child_samples',10,50,5),\n",
    "}\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=20)\n",
    "print(\"LightGBM: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.301%\n",
      "Precision: 51.214999999999996%\n",
      "Recall: 43.402%\n",
      "F1-score: 44.326%\n",
      "CPU times: total: 1h 58min 12s\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(max_depth=14, learning_rate=  0.4765834961973211, n_estimators = 480, \n",
    "                         num_leaves = 600, min_child_samples = 25)\n",
    "clf.fit(X,y)\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='accuracy')\n",
    "print(\"Accuracy: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='precision')\n",
    "print(\"Precision: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='recall')\n",
    "print(\"Recall: \"+ str(round(scores.mean(),5)*100)+\"%\")\n",
    "scores = cross_val_score(clf, X, y, cv=5,scoring='f1')\n",
    "print(\"F1-score: \"+ str(round(scores.mean(),5)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter optimization, the cross-validation accuracy has been improved from 98.438% to 98.477%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']), \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': abs(float(params['learning_rate'])),\n",
    "        \"num_leaves\": int(params['num_leaves']),\n",
    "        \"min_child_samples\": int(params['min_child_samples']),\n",
    "    }\n",
    "    clf = lgb.LGBMClassifier( **params)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test,predictions)\n",
    "    return {'loss':-score, 'status': STATUS_OK }\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 500, 20),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "    \"learning_rate\":hp.uniform('learning_rate', 0, 1),\n",
    "    \"num_leaves\":hp.quniform('num_leaves',100,2000,100),\n",
    "    \"min_child_samples\":hp.quniform('min_child_samples',10,50,5),\n",
    "}\n",
    "\n",
    "# Detect the optimal hyperparameter values\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50)\n",
    "print(\"LightGBM: Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.84100000000001%\n",
      "Precision: 99.381%\n",
      "Recall: 99.822%\n",
      "F1-score: 99.601%\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgb.LGBMClassifier(max_depth=35, learning_rate= 0.7925617918030913, n_estimators = 200, \n",
    "                         num_leaves = 200, min_child_samples = 25)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Accuracy: \"+str(round(accuracy_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Precision: \"+str(round(precision_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"Recall: \"+str(round(recall_score(y_test,predictions),5)*100)+\"%\")\n",
    "print(\"F1-score: \"+str(round(f1_score(y_test,predictions),5)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After hyperparameter optimization, the hold-out accuracy has been improved from 99.806% to 99.841%"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "196afc9dc7b5c9c3a7538bec70b0d7a0f8aad1956bc30b611308dbe5d44a1020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
